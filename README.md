# AI Trends Aggregator

A modern web application that aggregates and displays trending AI/ML repositories from GitHub and popular models from Hugging Face. Built with Next.js and featuring AI-powered content summarization.

## ğŸš€ Live Demo

- **Production (Auto-deploy)**: https://ex2-mauve-zeta.vercel.app/
- **Manual Deploy**: https://ex2-1oms.vercel.app/

## ğŸ“¸ Screenshots

### Main Application

![AI Trends Main Page](./public/main-page.jpg)
_The main page showing aggregated AI/ML trends from GitHub and Hugging Face with summarization features_

### Settings & Configuration

![Settings Page](./public/settings-page.jpg)
_Settings page for configuring AI providers and deployment options_

## âœ¨ Features

- **Dual Source Aggregation**: Fetches trending repositories from both GitHub and Hugging Face
- **AI-Powered Summaries**: Automatically generates concise summaries of project READMEs using multiple AI providers
- **Smart Content Processing**: Handles large README files with intelligent truncation
- **Multiple AI Providers**: Support for Groq, OpenAI, and Anthropic APIs
- **Real-time Data**: Caches data for performance while ensuring fresh content
- **Responsive Design**: Clean, modern UI that works on all devices
- **One-Click Deployment**: Built-in Vercel deployment trigger

## ğŸ› ï¸ Technology Stack

- **Frontend**: Next.js 16, React 19
- **Styling**: CSS Modules with modern design system
- **API Integration**: GitHub API, Hugging Face API
- **AI Services**: Groq, OpenAI, Anthropic
- **Deployment**: Vercel
- **Caching**: In-memory caching with TTL

## ğŸ“‹ Prerequisites

- Node.js 20.9.0 or higher
- npm, yarn, pnpm, or bun
- API keys for at least one AI provider:
  - [Groq API Key](https://console.groq.com/) (Recommended - Free tier available)
  - [OpenAI API Key](https://platform.openai.com/api-keys)
  - [Anthropic API Key](https://console.anthropic.com/)

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone <your-repo-url>
cd ai-trends-aggregator
```

### 2. Install dependencies

```bash
npm install
# or
yarn install
# or
pnpm install
# or
bun install
```

### 3. Set up environment variables (Optional)

For the deploy button feature, create a `.env.local` file:

```bash
VERCEL_DEPLOY_HOOK=your_vercel_deploy_hook_url
```

### 4. Run the development server

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

### 5. Configure AI Provider

1. Click the settings gear icon (âš™ï¸) in the top-right corner
2. Enter your API key for your preferred AI provider
3. Select the AI provider (Groq, OpenAI, or Anthropic)
4. Save your settings

## ğŸ—ï¸ Project Structure

```
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ anthropic/      # Anthropic summarization API
â”‚   â”‚   â”œâ”€â”€ feed/           # Main aggregation endpoint
â”‚   â”‚   â”œâ”€â”€ github/         # GitHub data fetching
â”‚   â”‚   â”œâ”€â”€ groq/           # Groq summarization API
â”‚   â”‚   â”œâ”€â”€ huggingface/    # Hugging Face data fetching
â”‚   â”‚   â”œâ”€â”€ openAI/         # OpenAI summarization API
â”‚   â”‚   â””â”€â”€ vercel/         # Deployment trigger API
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ summarizeCooldown.js  # Rate limiting for AI requests
â”‚   â”œâ”€â”€ settings/           # Settings page
â”‚   â”œâ”€â”€ globals.css         # Global styles
â”‚   â”œâ”€â”€ layout.js           # Root layout
â”‚   â””â”€â”€ page.jsx            # Home page
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ NewsCard.jsx        # Individual item card component
â”‚   â””â”€â”€ NewsCard.module.css # Card-specific styles
â””â”€â”€ public/                 # Static assets
```

## ğŸ”§ API Endpoints

### Public Endpoints

- `GET /api/feed` - Aggregated data from both GitHub and Hugging Face
- `GET /api/github` - GitHub repositories data
- `GET /api/huggingface` - Hugging Face models data

### Protected Endpoints (Require API Key)

- `POST /api/groq` - Groq AI summarization
- `POST /api/openai` - OpenAI summarization
- `POST /api/anthropic` - Anthropic summarization

### Utility Endpoints

- `POST /api/vercel` - Trigger Vercel deployment
- `POST /api/github` - Fetch GitHub README content
- `POST /api/huggingface` - Fetch Hugging Face README content

## âš™ï¸ Configuration

### AI Provider Settings

The application supports three AI providers for content summarization:

1. **Groq** (Recommended)
   - Fast and cost-effective
   - Uses Llama-3.1-8b-instant model
   - Generous free tier

2. **OpenAI**
   - Uses GPT-4o-mini model
   - High-quality summaries
   - Requires paid API key

3. **Anthropic**
   - Uses Claude-3-5-haiku model
   - Excellent for technical content
   - Requires paid API key

### Caching Strategy

- **Data Caching**: 5-minute TTL for API responses
- **AI Summaries**: Cached to prevent redundant API calls
- **Rate Limiting**: 2-second cooldown between summarization requests

## ğŸš€ Deployment

### Automatic Deployment (Recommended)

The app automatically deploys to https://ex2-mauve-zeta.vercel.app/ when you push commits to your main branch.

### Manual Deployment

Use the built-in deployment button:

1. Go to Settings page
2. Click "Deploy to Vercel"
3. Monitor deployment at https://ex2-1oms.vercel.app/

### Environment Variables for Production

If you want to use the manual deploy feature, set this in your Vercel dashboard:

```bash
VERCEL_DEPLOY_HOOK=your_vercel_deploy_hook_url
```

## ğŸ¯ Usage

### Main Interface

As shown in the main page screenshot above:

1. **Browse Trends**: The home page displays trending AI/ML projects from both GitHub and Hugging Face
   - GitHub repositories are marked with a "GitHub" badge
   - Hugging Face models are marked with a "Hugging Face" badge
   - Each item shows stars/likes count, programming language, and owner information

2. **Get Summaries**: Click "Summarize" on any item to get an AI-generated summary
   - The app intelligently fetches README files and processes them
   - Large files are automatically truncated for optimal AI processing
   - Summaries are cached to avoid repeated API calls

3. **Filter Content**: Items are automatically sorted by popularity (stars/likes)
4. **Access Projects**: Click "Open" to visit the original repository or model page

### Configuration

As shown in the settings page screenshot:

1. **AI Provider Setup**: Choose from Groq, OpenAI, or Anthropic
2. **API Key Management**: Securely store your API keys locally
3. **One-Click Deployment**: Deploy your changes directly from the settings page

## ğŸ” How It Works

1. **Data Aggregation**: The `/api/feed` endpoint fetches trending repositories and models from both platforms
2. **Content Processing**: When you click "Summarize", the app:
   - Attempts to fetch the project's README file
   - Handles large files with intelligent truncation
   - Falls back to AI knowledge if README is unavailable
   - Generates a concise 3-line summary using your chosen AI provider

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ› Troubleshooting

### Common Issues

1. **"Please enter an API key in Settings"**
   - Go to Settings and configure your AI provider API key

2. **Summarization fails**
   - Check your API key is correct
   - Verify you have sufficient API credits
   - Some content may be restricted or too short to summarize

3. **No items loading**
   - Check your internet connection
   - GitHub/Hugging Face APIs may be temporarily unavailable

4. **Deploy button not working**
   - Ensure `VERCEL_DEPLOY_HOOK` environment variable is set in Vercel

## ğŸ“Š Performance

- **Caching**: 5-minute cache reduces API calls and improves response times
- **Concurrent Fetching**: Parallel requests to GitHub and Hugging Face
- **Smart Truncation**: Large README files are intelligently truncated to stay within AI model limits
- **Error Handling**: Graceful fallbacks ensure the app continues working even if some services are unavailable

---

Built with â¤ï¸ using Next.js and powered by AI
